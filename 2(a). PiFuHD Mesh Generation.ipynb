{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2(a). PiFuHD Mesh Generation.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1TfPAtL4CyZw"
      },
      "source": [
        "## More Info\n",
        "- Paper: https://arxiv.org/pdf/2004.00452.pdf\n",
        "- Repo: https://github.com/facebookresearch/pifuhd\n",
        "- Project Page: https://shunsukesaito.github.io/PIFuHD/\n",
        "- 1-minute/5-minute Presentation https://www.youtube.com/embed/LWDGR5v3-3o"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8vZaAyhUJ9QC"
      },
      "source": [
        "## Requirements\n",
        "- Python 3\n",
        "- PyTorch tested on 1.4.0\n",
        "- json\n",
        "- PIL\n",
        "- skimage\n",
        "- tqdm\n",
        "- numpy\n",
        "- cv2"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QvQm-A8ESKb2"
      },
      "source": [
        "## Configure input data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "cOvj7pWCgr3G",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d017dd0a-c3de-42f9-9763-468156f5396d"
      },
      "source": [
        " from google.colab import drive \n",
        " drive.mount('/content/drive/')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Mounted at /content/drive/\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "-ukvbNnAg-rW",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e44ac5ac-9706-4fe3-d1c4-1037c7306d12"
      },
      "source": [
        "cd \"/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LogRQqwVBxeu"
      },
      "source": [
        "#Comment this cell after first run as it will save the desired code libraries to your Google Drive\n",
        "!git clone https://github.com/facebookresearch/pifuhd"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hTrhJ9M2B1D6"
      },
      "source": [
        "#Comment this cell after first run as it will save the desired code libraries to your Google Drive\n",
        "!git clone https://github.com/Daniil-Osokin/lightweight-human-pose-estimation.pytorch.git"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9SI7Ye1JfIim"
      },
      "source": [
        "**Uploading Images**. Currently PNG, JPEG files are supported."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "N6UDTzYgv9b6"
      },
      "source": [
        "import os\n",
        "\n",
        "files=[]\n",
        "path = '/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/'\n",
        "for file in os.listdir(path):\n",
        "  files.append(file)\n",
        "files.sort()"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "K8A_xMeX4DSz",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "17d17b19-0b50-4a08-a3e1-fb4c54777ac1"
      },
      "source": [
        "files"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['bvInside.jpg',\n",
              " 'bvOutside.jpg',\n",
              " 'bvOutsideU.jpg',\n",
              " 'fvInside.jpg',\n",
              " 'fvOutside.jpg',\n",
              " 'fvOutsideU.jpg',\n",
              " 'lsvInside.jpg',\n",
              " 'lsvOutsideU.jpg',\n",
              " 'lvOutside.jpg',\n",
              " 'rsvInside.jpg',\n",
              " 'rsvOutside.jpg',\n",
              " 'rsvOutsideU.jpg']"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JbVmda9J5TDL"
      },
      "source": [
        "## Preprocess (for cropping image)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "F-vYklhI5dab",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e6c18795-2441-4460-ef0b-92bd7a4c79df"
      },
      "source": [
        "cd \"lightweight-human-pose-estimation.pytorch\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/lightweight-human-pose-estimation.pytorch\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YNwya2tsB-li"
      },
      "source": [
        "#Comment this cell after first run as it will save the desired code libraries to your Google Drive\n",
        "!wget https://download.01.org/opencv/openvino_training_extensions/models/human_pose_estimation/checkpoint_iter_370000.pth"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "PdRcDXe38lHB"
      },
      "source": [
        "import torch\n",
        "import cv2\n",
        "import numpy as np\n",
        "from models.with_mobilenet import PoseEstimationWithMobileNet\n",
        "from modules.keypoints import extract_keypoints, group_keypoints\n",
        "from modules.load_state import load_state\n",
        "from modules.pose import Pose, track_poses\n",
        "import demo\n",
        "\n",
        "def get_rect(net, images, height_size):\n",
        "    net = net.eval()\n",
        "\n",
        "    stride = 8\n",
        "    upsample_ratio = 4\n",
        "    num_keypoints = Pose.num_kpts\n",
        "    previous_poses = []\n",
        "    delay = 33\n",
        "    for image in images:\n",
        "        rect_path = image.replace('.%s' % (image.split('.')[-1]), '_rect.txt')\n",
        "        img = cv2.imread(image, cv2.IMREAD_COLOR)\n",
        "        orig_img = img.copy()\n",
        "        orig_img = img.copy()\n",
        "        heatmaps, pafs, scale, pad = demo.infer_fast(net, img, height_size, stride, upsample_ratio, cpu=False)\n",
        "\n",
        "        total_keypoints_num = 0\n",
        "        all_keypoints_by_type = []\n",
        "        for kpt_idx in range(num_keypoints):  # 19th for bg\n",
        "            total_keypoints_num += extract_keypoints(heatmaps[:, :, kpt_idx], all_keypoints_by_type, total_keypoints_num)\n",
        "\n",
        "        pose_entries, all_keypoints = group_keypoints(all_keypoints_by_type, pafs)\n",
        "        for kpt_id in range(all_keypoints.shape[0]):\n",
        "            all_keypoints[kpt_id, 0] = (all_keypoints[kpt_id, 0] * stride / upsample_ratio - pad[1]) / scale\n",
        "            all_keypoints[kpt_id, 1] = (all_keypoints[kpt_id, 1] * stride / upsample_ratio - pad[0]) / scale\n",
        "        current_poses = []\n",
        "\n",
        "        rects = []\n",
        "        for n in range(len(pose_entries)):\n",
        "            if len(pose_entries[n]) == 0:\n",
        "                continue\n",
        "            pose_keypoints = np.ones((num_keypoints, 2), dtype=np.int32) * -1\n",
        "            valid_keypoints = []\n",
        "            for kpt_id in range(num_keypoints):\n",
        "                if pose_entries[n][kpt_id] != -1.0:  # keypoint was found\n",
        "                    pose_keypoints[kpt_id, 0] = int(all_keypoints[int(pose_entries[n][kpt_id]), 0])\n",
        "                    pose_keypoints[kpt_id, 1] = int(all_keypoints[int(pose_entries[n][kpt_id]), 1])\n",
        "                    valid_keypoints.append([pose_keypoints[kpt_id, 0], pose_keypoints[kpt_id, 1]])\n",
        "            valid_keypoints = np.array(valid_keypoints)\n",
        "            \n",
        "            if pose_entries[n][10] != -1.0 or pose_entries[n][13] != -1.0:\n",
        "              pmin = valid_keypoints.min(0)\n",
        "              pmax = valid_keypoints.max(0)\n",
        "\n",
        "              center = (0.5 * (pmax[:2] + pmin[:2])).astype(np.int)\n",
        "              radius = int(0.65 * max(pmax[0]-pmin[0], pmax[1]-pmin[1]))\n",
        "            elif pose_entries[n][10] == -1.0 and pose_entries[n][13] == -1.0 and pose_entries[n][8] != -1.0 and pose_entries[n][11] != -1.0:\n",
        "              # if leg is missing, use pelvis to get cropping\n",
        "              center = (0.5 * (pose_keypoints[8] + pose_keypoints[11])).astype(np.int)\n",
        "              radius = int(1.45*np.sqrt(((center[None,:] - valid_keypoints)**2).sum(1)).max(0))\n",
        "              center[1] += int(0.05*radius)\n",
        "            else:\n",
        "              center = np.array([img.shape[1]//2,img.shape[0]//2])\n",
        "              radius = max(img.shape[1]//2,img.shape[0]//2)\n",
        "\n",
        "            x1 = center[0] - radius\n",
        "            y1 = center[1] - radius\n",
        "\n",
        "            rects.append([x1, y1, 2*radius, 2*radius])\n",
        "\n",
        "        np.savetxt(rect_path, np.array(rects), fmt='%d')"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "M6cGZD6f6IaY",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3437ba98-f97a-4c5a-f7cb-12f9eabd33b3"
      },
      "source": [
        "net = PoseEstimationWithMobileNet()\n",
        "checkpoint = torch.load('checkpoint_iter_370000.pth', map_location='cpu')\n",
        "load_state(net, checkpoint)\n",
        "\n",
        "for file in files:\n",
        "  image_path= '/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/%s' % file\n",
        "  print(image_path)\n",
        "  get_rect(net.cuda(), [image_path], 512)"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/bvInside.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/bvOutside.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/bvOutsideU.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/fvInside.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/fvOutside.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/fvOutsideU.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/lsvInside.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/lsvOutsideU.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/lvOutside.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/rsvInside.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/rsvOutside.jpg\n",
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/sample_images/rsvOutsideU.jpg\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y0rgMInwTt0s"
      },
      "source": [
        "## Download the Pretrained Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UrIcZweSNRFI",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "82ecf7fb-9944-49b5-baa4-af637c2a2e87"
      },
      "source": [
        "cd '/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd/'"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/TCD Sem2/Augmented Reality/Project/pifuhd\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6ZIEst8DCLJC"
      },
      "source": [
        "#Comment this cell after first run as it will save the desired code libraries to your Google Drive\n",
        "!sh ./scripts/download_trained_model.sh"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6heKcA-0QEBw"
      },
      "source": [
        "## Run PIFuHD!\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5995t2PnQTmG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7491a6ba-0881-4532-9229-89f51825ff53"
      },
      "source": [
        "# Warning: all images with the corresponding rectangle files under -i will be processed. \n",
        "!python -m apps.simple_test -r 256 --use_rect -i ./sample_images/\n",
        "\n",
        "# seems that 256 is the maximum resolution that can fit into Google Colab. \n",
        "# If you want to reconstruct a higher-resolution mesh, please try with your own machine. "
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Resuming from  ./checkpoints/pifuhd.pt\n",
            "Warning: opt is overwritten.\n",
            "test data size:  12\n",
            "initialize network with normal\n",
            "initialize network with normal\n",
            "generate mesh (test) ...\n",
            "  0% 0/12 [00:00<?, ?it/s]./results/pifuhd_final/recon/result_bvInside_256.obj\n",
            "  8% 1/12 [00:06<01:14,  6.80s/it]./results/pifuhd_final/recon/result_bvOutside_256.obj\n",
            " 17% 2/12 [00:12<01:04,  6.40s/it]./results/pifuhd_final/recon/result_bvOutsideU_256.obj\n",
            " 25% 3/12 [00:17<00:55,  6.17s/it]./results/pifuhd_final/recon/result_fvInside_256.obj\n",
            " 33% 4/12 [00:23<00:47,  6.00s/it]./results/pifuhd_final/recon/result_fvOutside_256.obj\n",
            " 42% 5/12 [00:28<00:40,  5.81s/it]./results/pifuhd_final/recon/result_fvOutsideU_256.obj\n",
            " 50% 6/12 [00:34<00:34,  5.75s/it]./results/pifuhd_final/recon/result_lsvInside_256.obj\n",
            " 58% 7/12 [00:40<00:28,  5.80s/it]./results/pifuhd_final/recon/result_lsvOutsideU_256.obj\n",
            " 67% 8/12 [00:46<00:23,  5.79s/it]./results/pifuhd_final/recon/result_lvOutside_256.obj\n",
            " 75% 9/12 [00:51<00:17,  5.80s/it]./results/pifuhd_final/recon/result_rsvInside_256.obj\n",
            " 83% 10/12 [00:58<00:11,  5.91s/it]./results/pifuhd_final/recon/result_rsvOutside_256.obj\n",
            " 92% 11/12 [01:04<00:05,  5.90s/it]./results/pifuhd_final/recon/result_rsvOutsideU_256.obj\n",
            "100% 12/12 [01:09<00:00,  5.82s/it]\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}